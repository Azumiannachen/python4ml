{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy, A Vanilla Neural Network & Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Vanilla SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy deals with matrices.\n",
    "\n",
    "### Why matrices?\n",
    "\n",
    "#### Reason to action\n",
    "In the simple single number world(scalar), many problem can be boil down into \"if it rains, I'll stay at home on saturday\". Single reason cause the single action.\n",
    "\n",
    "#### Reasons to actions\n",
    "Say you might stay at home on saturday, you might go to the movie, you might go to the bar, you might go to the park, you might visit the museum.etc.  Each possible action you might choose are from tens of the reasons. The possibility of you going to museum can be decided from all kinds of things, weather will be a heavy weight reason, or if there is a friend who can explain good history/art, or if there is an attractive one who's willing to go there and looking for someone to accompany him/her, or if your personality thinks going to museum is a dumb idea. Of course, \"if your personality thinks going to museum is a dumb idea\" will also cause certain decision making about if you are going to a bar.\n",
    "\n",
    "#### Reasons to actions in table format\n",
    "Since there might be 10 possible actions you might take, and there might be 1000 possible condition, reasons for you to choose between 10 actions. How about I'll save some of the typing, and give out a mapping table, about how of the each reason that's going to affect each of your action choosing. So there will be what? a table of 1000 * 10, right. each contains a number about how strong is the each of the given condition(x) will affect your action(y).\n",
    "\n",
    "Matrix is a sexy way to name the table situation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cifar 10 example is modified from [here](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1000), (64, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = np.random.rand(N, D_in)\n",
    "y = np.random.rand(N, D_out)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 100), (100, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.rand(D_in, H)\n",
    "w2 = np.random.rand(H, D_out)\n",
    "\n",
    "w1.shape,w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/Users/zhangxiaochen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in square\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "100%|██████████| 500/500 [00:00<00:00, 540.58it/s]\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-6\n",
    "from tqdm import trange\n",
    "\n",
    "for i in trange(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h,0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights, \n",
    "    # the simplist form of gradient-descent-based optimizer\n",
    "    w1-=grad_w1 * learning_rate\n",
    "    w2-=grad_w2 * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From numpy to pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is like numpy, but:\n",
    "\n",
    "* A simple pytorch neural network example goes very much **like the vanilla numpy nn** you just did, but **simpler**. Actually the numpy example above is from [this official pytorch tutorial](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "* It can calculate gradients **automatically**. In one single line of coding\n",
    "* It has various kinds of ready to use optimizers.\n",
    "* It can run on **gpu**, in most cases faster than tensorflow, also it can perform matrix/tensor operations on gpu even if it's not machine learning task.\n",
    "* It includes many powerful and easy to use tools to process data, to wash & slice them up nice and clean.\n",
    "* It has a logo and cooler name.\n",
    "\n",
    "And more, the summarization is not conclusive.\n",
    "\n",
    "Now it sounds less daunting when I tell you this is the most trending deep learning library for top research labs and universities, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([.5,.5,.5],[.5,.5,.5]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DL = False\n",
    "train_set=CIFAR10(\"./data\",train=True,download=DL,transform=transform)\n",
    "test_set=CIFAR10(\"./data\",train=False,download=DL,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=160,num_workers=2,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=160,num_workers=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter=iter(train_loader)\n",
    "test_iter=iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample=train_iter.__next__()  # structure: x,batch_size, channel, height, width ;y, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160, 3, 32, 32]), torch.Size([160]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].size(),sample[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangxiaochen/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['msyh'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d506160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_ = 4\n",
    "imshow(sample[0][i_]),print(classes[sample[1][i_]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class basic_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(basic_cnn,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32,(3,3),stride=(1,1),padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,64,(3,3),stride=(1,1),padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64,64,(3,3),stride=(1,1),padding=1)\n",
    "        self.conv4 = nn.Conv2d(64,128,(3,3),stride=(2,2),padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,128,(3,3),stride=(1,1),padding=1)\n",
    "        self.conv6 = nn.Conv2d(128,128,(3,3),stride=(2,2),padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(128,128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Conv layers\n",
    "        \n",
    "        # Conv block1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(self.bn1(F.relu(self.conv2(x))))\n",
    "        \n",
    "        # Conv blcok2\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(self.bn2(F.relu(self.conv4(x))))\n",
    "        \n",
    "        # Conv block3\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(self.bn3(F.relu(self.conv6(x))))\n",
    "        \n",
    "        x = x.view(-1,128)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn_fc1(x)\n",
    "        return F.softmax(self.fc2(x))\n",
    "    \n",
    "cnn = basic_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of all the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444042"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(np.product(list(l.size())) for l in cnn.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cnn.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "epoch0_b0 \tloss:\t0.09 \t acc:\t0.036: 100%|██████████| 1/1 [00:02<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "window=20\n",
    "CUDA=torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    cnn.cuda()\n",
    "for epoch in range(1):\n",
    "    train_iter=iter(train_loader)\n",
    "    test_iter=iter(test_loader)\n",
    "    running_loss,running_acc = 0,0\n",
    "    \n",
    "    t=trange(len(train_loader))\n",
    "    for i in t:\n",
    "        # get data\n",
    "        x,y = train_iter.__next__()\n",
    "        if CUDA:\n",
    "            x,y = Variable(x.cuda()),Variable(y.cuda())\n",
    "        else:\n",
    "            x,y = Variable(x),Variable(y)\n",
    "        \n",
    "        # make the gradient zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass & calc loss\n",
    "        y_ = cnn(x)\n",
    "        loss = loss_func(y_,y)\n",
    "        if CUDA:\n",
    "            acc=np.mean(np.equal(y_.data.cpu().numpy().argmax(axis=-1),y.data.cpu().numpy()))\n",
    "        else:\n",
    "            acc=np.mean(np.equal(y_.data.numpy().argmax(axis=-1),y.data.numpy()))\n",
    "        \n",
    "        # calculate the gradient\n",
    "        loss.backward()\n",
    "        # update the gradient to the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        running_acc += acc\n",
    "        \n",
    "        if i%window == 0:\n",
    "            loss_,acc_=running_loss/window,running_acc/window\n",
    "            t.set_description(\"epoch%s_b%s \\tloss:\\t%.2f \\t acc:\\t%.3f\"%(epoch,i,loss_,acc_))\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "    \n",
    "    # Validation score\n",
    "    loss_t,acc_t=0,0\n",
    "    for i in range(len(test_loader)):\n",
    "        x,y = test_iter.__next__()\n",
    "        if CUDA:\n",
    "            x,y = Variable(x.cuda()),Variable(y.cuda())\n",
    "        else:\n",
    "            x,y = Variable(x),Variable(y)\n",
    "        y_ = cnn(x)\n",
    "        loss_t += loss_func(y_,y)\n",
    "        if CUDA:\n",
    "            acc_t +=np.mean(np.equal(y_.data.cpu().numpy().argmax(axis=-1),y.data.cpu().numpy()))\n",
    "        else:\n",
    "            acc_t +=np.mean(np.equal(y_.data.numpy().argmax(axis=-1),y.data.numpy()))\n",
    "    print(\"[Validation]\\tloss:\\t%.2f\\tacc:\\t%.3f\\t[%s]\"%(loss_t/len(test_loader),\n",
    "                                                         acc_t/len(test_loader)),\n",
    "                                                          \"overfitting\" if loss_t/len(test_loader)>loss_ else \"Not overfitting\")\n",
    "print(\"=\"*60,\"\\n\",\"Training finished\",\"\\n\",\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tiny Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are good with python class and the input/output dimensions of convolution, try the next example for challenge.\n",
    "\n",
    "DenseNet is from last year's image classification research. It was considered most successful breakthrough on the subject on 2017\n",
    "\n",
    "It takes full advantage of skip connection, but its skip connections work in a densely connected way.\n",
    "\n",
    "It doesn't overfit easily, and can achieve the same benchmark like resnet but with fewer parameters\n",
    "\n",
    "See the [DenseNet Paper](https://arxiv.org/abs/1608.06993)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a tiny densenet for cifar 10 problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class basic_dn(nn.Module):\n",
    "    def __init__(self,fil_nb=16,block_nb=4,conv_in_block=5):\n",
    "        \"\"\"\n",
    "        basic tiny dense net structure with pytorch\n",
    "        \"\"\"\n",
    "        super(basic_dn,self).__init__()\n",
    "        self.block_nb = block_nb\n",
    "        self.conv_in_block = conv_in_block\n",
    "        # if the stride is (1,1), we set padding to 0, and the map won't reduce size\n",
    "        self.conv0 = nn.Conv2d(3,fil_nb,(1,1),stride=(1,1),padding=0)\n",
    "        for i in range(self.block_nb):\n",
    "            for j in range(self.conv_in_block-1):\n",
    "                setattr(self,\"dn_%s_%s\"%(i,j),nn.Conv2d(fil_nb+j*fil_nb,\n",
    "                                  fil_nb+j*fil_nb,\n",
    "                                  (3,3),stride=(1,1),padding=1))\n",
    "            setattr(self,\"dn_%s_%s\"%(i,self.conv_in_block-1),\n",
    "                    nn.Conv2d(fil_nb*(self.conv_in_block),fil_nb*2,(1,1),stride=(1,1),padding=0))\n",
    "            fil_nb *= 2\n",
    "            setattr(self,\"bn_%s\"%(i),nn.BatchNorm2d(fil_nb))\n",
    "             \n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024,512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Conv layers\n",
    "        x = self.conv0(x)\n",
    "        #print(\"after0\\t\",x.size())\n",
    "        for i in range(self.block_nb):\n",
    "            xc = x\n",
    "            \n",
    "            for j in range(self.conv_in_block-1):\n",
    "                #print(\"dn_%s_%s\\t\"%(i,j),getattr(self,\"dn_%s_%s\"%(i,j)).weight.size())\n",
    "                xc = torch.cat([getattr(self,\"dn_%s_%s\"%(i,j))(xc),x],dim=1)\n",
    "                #print(\"dn_%s_%s after concat\\t\"%(i,j),xc.size())\n",
    "                xc = F.relu(xc)\n",
    "                \n",
    "            x = getattr(self,\"dn_%s_%s\"%(i,self.conv_in_block-1))(xc)\n",
    "            #print(\"dn_%s_%s after \\t\"%(i,j),x.size())\n",
    "            x = F.relu(x)\n",
    "            x = getattr(self,\"bn_%s\"%(i))(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1,1024)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn_fc1(x)\n",
    "        return F.softmax(self.fc2(x))\n",
    "    \n",
    "dn = basic_dn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6627658"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(np.product(list(l.size())) for l in dn.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(dn.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basic_dn(\n",
       "  (conv0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (dn_0_0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_0_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_0_2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_0_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_0_4): Conv2d(80, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (dn_1_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_1_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_1_2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_1_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_1_4): Conv2d(160, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (dn_2_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_2_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_2_2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_2_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_2_4): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (dn_3_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_3_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_3_2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_3_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dn_3_4): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (bn_fc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7ff6566774a8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]\u001b[A/home/zhangxiaochen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   0%|          | 0/313 [00:00<?, ?it/s]\u001b[A\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   0%|          | 1/313 [00:00<00:32,  9.54it/s]\u001b[A\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   1%|          | 3/313 [00:00<00:29, 10.57it/s]\u001b[A\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   2%|▏         | 5/313 [00:00<00:28, 10.79it/s]\u001b[A\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   2%|▏         | 7/313 [00:00<00:27, 10.95it/s]\u001b[A\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   3%|▎         | 9/313 [00:00<00:27, 10.99it/s]\u001b[A\n",
      "epoch_0_batch_0 \tloss:\t0.12 \t acc:\t0.004:   4%|▎         | 11/313 [00:00<00:27, 11.06it/s]\u001b[A\n",
      "Exception in thread Thread-72:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/zhangxiaochen/anaconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "epoch_0_batch_300 \tloss:\t1.91 \t acc:\t0.553: 100%|██████████| 313/313 [00:27<00:00, 11.48it/s]\n",
      "epoch_1_batch_300 \tloss:\t1.82 \t acc:\t0.640: 100%|██████████| 313/313 [00:27<00:00, 11.52it/s]\n",
      "epoch_2_batch_300 \tloss:\t1.78 \t acc:\t0.684: 100%|██████████| 313/313 [00:27<00:00, 11.49it/s]\n",
      "epoch_3_batch_300 \tloss:\t1.74 \t acc:\t0.718: 100%|██████████| 313/313 [00:27<00:00, 11.49it/s]\n",
      "epoch_4_batch_300 \tloss:\t1.72 \t acc:\t0.739: 100%|██████████| 313/313 [00:27<00:00, 11.43it/s]\n",
      "epoch_5_batch_300 \tloss:\t1.69 \t acc:\t0.775: 100%|██████████| 313/313 [00:27<00:00, 11.39it/s]\n",
      "epoch_6_batch_300 \tloss:\t1.67 \t acc:\t0.792: 100%|██████████| 313/313 [00:27<00:00, 11.39it/s]\n",
      "epoch_7_batch_300 \tloss:\t1.67 \t acc:\t0.794: 100%|██████████| 313/313 [00:27<00:00, 11.39it/s]\n",
      "epoch_8_batch_300 \tloss:\t1.64 \t acc:\t0.821: 100%|██████████| 313/313 [00:27<00:00, 11.38it/s]\n",
      "epoch_9_batch_300 \tloss:\t1.66 \t acc:\t0.801: 100%|██████████| 313/313 [00:27<00:00, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================ \n",
      " Training finished \n",
      " ============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "window=20\n",
    "CUDA=True\n",
    "if CUDA:\n",
    "    dn.cuda()\n",
    "for epoch in range(10):\n",
    "    train_iter=iter(train_loader)\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    \n",
    "    t=trange(len(train_loader))\n",
    "    for i in t:\n",
    "        # get data\n",
    "        x,y = train_iter.__next__()\n",
    "        if CUDA:\n",
    "            x,y = Variable(x.cuda()),Variable(y.cuda())\n",
    "        else:\n",
    "            x,y = Variable(x),Variable(y)\n",
    "        \n",
    "        # make the gradient zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass & calc loss\n",
    "        y_ = dn(x)\n",
    "        loss = loss_func(y_,y)\n",
    "        if CUDA:\n",
    "            acc=np.mean(np.equal(y_.data.cpu().numpy().argmax(axis=-1),y.data.cpu().numpy()))\n",
    "        else:\n",
    "            acc=np.mean(np.equal(y_.data.numpy().argmax(axis=-1),y.data.numpy()))\n",
    "        \n",
    "        # calculate the gradient\n",
    "        loss.backward()\n",
    "        # update the gradient to the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        running_acc += acc\n",
    "        \n",
    "        if i%window == 0:\n",
    "            t.set_description(\"epoch_%s_batch_%s \\tloss:\\t%.2f \\t acc:\\t%.3f\"%(epoch,i,running_loss/window,running_acc/window))\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "        \n",
    "print(\"=\"*60,\"\\n\",\"Training finished\",\"\\n\",\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
